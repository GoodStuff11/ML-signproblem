\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{physics}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
\lstset{style=mystyle}

\title{Documentation for \texttt{ed\_optimization.jl}}
\author{Documentation generated by Antigravity}
\date{\today}

\begin{document}

\maketitle

\section{Overview}

The \texttt{ed\_optimization.jl} script provides a framework for optimizing unitary transformations $U(\theta)$ to map an initial quantum state $\ket{\psi_{\text{start}}}$ to a target state $\ket{\psi_{\text{target}}}$. The primary goal is to maximize the overlap (fidelity) between the evolved state and the target state:
\begin{equation}
    F(\theta) = |\bra{\psi_{\text{target}}} U(\theta) \ket{\psi_{\text{start}}}|^2
\end{equation}
Equivalently, it minimizes the loss function $L(\theta) = 1 - F(\theta)$.

The unitary is typically parameterized as $U(\theta) = \exp(\sum_k \theta_k A_k)$, where $A_k$ are anti-Hermitian operators (or $i \times$ Hermitian operators).

\section{Core Functions}

\subsection{\texttt{optimize\_unitary}}

This is the main entry point for the optimization process. It employs an iterative strategy to build the unitary transformation.

\textbf{Key Arguments:}
\begin{itemize}
    \item \texttt{state1}: Initial state vector.
    \item \texttt{state2}: Target state vector.
    \item \texttt{indexer}: Object handling the indexing of the basis states.
    \item \texttt{optimization\_scheme}: Vector of integers (e.g., \texttt{[1, 2]}) specifying the order of operators to add. Order 1 corresponds to one-body operators, Order 2 to two-body, etc.
    \item \texttt{optimization}: Symbol specifying the gradient method (e.g., \texttt{:gradient}, \texttt{:adjoint\_gradient}).
\end{itemize}

\textbf{Algorithm Flow:}
\begin{enumerate}
    \item Iterate through \texttt{optimization\_scheme}.
    \item For each order, generate a pool of candidate operators (e.g., all 1-body operators).
    \item Initialize parameters $\theta$.
    \item optimize $\theta$ to minimize $L(\theta)$ using the specified optimization method.
    \item Add the optimized term to the list of fixed compute matrices.
    \item Proceed to the next order, optimizing new parameters on top of the fixed previous unitary.
\end{enumerate}

\subsection{\texttt{test\_map\_to\_state}}

A wrapper function that assumes a sequence of states (e.g., ground states of a Hamiltonian at different parameters) and optimizes mappings between consecutive states. It iteratively calls \texttt{optimize\_unitary}.

\section{Loss Functions and Gradient Methods}

The script implements several methods for computing the loss and its gradient, offering a trade-off between speed and accuracy.

\subsection{\texttt{fast\_loss}}
Computes the loss using the Krylov subspace method (\texttt{expv}) for matrix exponentiation. 
\begin{enumerate}
    \item Variables are updated.
    \item Matrix $A = \sum \theta_k A_k$ is constructed.
    \item $\ket{\psi'} = \exp(A) \ket{\psi_{\text{start}}}$ is computed using \texttt{ExponentialUtilities.expv}.
    \item Loss is calculated as $1 - |\braket{\psi_{\text{target}}}{\psi'}|^2$.
\end{enumerate}
This method does \textbf{not} provide gradients and is used with gradient-free optimizers or for quick evaluation.

\subsection{\texttt{zygote\_loss}}
Uses \texttt{Zygote} automatic differentiation to compute gradients.
\begin{itemize}
    \item Uses dense matrix exponentiation \texttt{exp(Matrix(A))}.
    \item Suitable for small system sizes where dense matrices fit in memory and \texttt{exp} is fast.
\end{itemize}

\subsection{\texttt{approximate\_trotter\_grad\_loss}}
Implements a custom backpropagation through a Trotter-like approximation of the exponential.
\begin{itemize}
    \item Approximates $e^A \approx (I + A/N)^N$.
    \item Performs a forward pass to store intermediate states $\ket{r_k} = (I + A/N)^k \ket{\psi_{\text{start}}}$.
    \item Performs a backward pass to compute gradients by accumulating contributions from each layer.
    \item \textbf{Pros:} Faster than full AD for sparse matrices.
    \item \textbf{Cons:} Approximate gradient; accuracy depends on $N$ (hardcoded to 100).
\end{itemize}

\subsection{\texttt{adjoint\_loss}}
The most sophisticated method, utilizing the ``adjoint state method'' or ``quadrature gradient'' for exact matrix calibration. It defines a custom \texttt{rrule} for \texttt{ChainRulesCore}.

\textbf{Forward Pass:}
Computes loss using efficient \texttt{expv}.

\textbf{Backward Pass (Gradient):}
We need $\frac{\partial F}{\partial \theta_k}$. Using the formula:
\begin{equation}
    \frac{\partial}{\partial \theta} e^{A(\theta)} = \int_0^1 e^{(1-s)A} \frac{\partial A}{\partial \theta} e^{sA} ds
\end{equation}
The gradient component is computed as:
\begin{equation}
    \frac{\partial F}{\partial \theta_k} \propto \text{Re} \left( \braket{\psi'}{\psi_{\text{target}}} \int_0^1 \bra{\psi_{\text{target}}} e^{(1-s)A} A_k e^{sA} \ket{\psi_{\text{start}}} ds \right)
\end{equation}

The integral is evaluated using Simpson's rule:
\begin{enumerate}
    \item \textbf{Forward Checkpoints:} $|\phi(s)\rangle = e^{sA} |\psi_{\text{start}}\rangle$ are computed for $s \in [0, 1]$.
    \item \textbf{Backward Checkpoints:} $|\chi(s)\rangle = e^{-(1-s)A^\dagger} |\psi_{\text{target}}\rangle$ are computed matrix-free or using \texttt{expv}.
    \item The integral is approximated as a weighted sum $\sum_j w_j \bra{\chi(s_j)} A_k \ket{\phi(s_j)}$.
\end{enumerate}

This method is efficient ($O(1)$ memory wrt parameters, $O(N)$ time) and accurate.

\section{Implementation Details}
\begin{itemize}
    \item \textbf{Sparse Matrices:} The code heavily relies on \texttt{SparseMatrixCSC} to handle large Hilbert spaces efficiently.
    \item \textbf{Symmetry:} If \texttt{use\_symmetry=true}, operators are grouped by symmetry sectors to reduce the number of optimization parameters.
\end{itemize}

\end{document}
